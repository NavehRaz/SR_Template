import SRmodellib_lifelines as srl
import numpy as np
from numba import jit
from joblib import Parallel, delayed
import deathTimesDataSet as dtds
import os
import sr_mcmc as srmc


jit_nopython = True

"""
After implementing your class, change sr_mcmc.model so it calls your class instead of the default one and uses your metric function.
"""


class My_SR(srl.SR_lf):
    def __init__(self, eta, beta, kappa, epsilon, xc, npeople, nsteps, t_end, t_start=0, tscale='years', external_hazard=np.inf, time_step_multiplier=1, parallel=False, bandwidth=3, heun=False):
        """
        If you want to add parameters to the __init__ method, you can do so here before the call to super().__init__. if you add beta2 as aparameter for example then
        add self.beta2=beta2 here.
        """
        
        #this is the call to my class, do not modify it. also, do not earase any of the parameters I cal here unless you give them a default value or somehting
        super().__init__(eta, beta, kappa, epsilon, xc, npeople, nsteps, t_end, t_start, tscale, external_hazard, time_step_multiplier, parallel, bandwidth, heun)




    def calc_death_times(self):
        #modify to add your things  here
        s = len(self.t)
        dt = self.t[1]-self.t[0]
        sdt = np.sqrt(dt)
        t = self.t

        if self.parallel:
            death_times, events =death_times_accelerator2(s,dt,t,self.eta,self.beta,self.kappa,self.epsilon,self.xc,sdt,self.npeople,self.external_hazard,self.time_step_multiplier)
        else:
            death_times, events =death_times_accelerator(s,dt,t,self.eta,self.beta,self.kappa,self.epsilon,self.xc,sdt,self.npeople,self.external_hazard,self.time_step_multiplier)

        return np.array(death_times), np.array(events)
    

    
def My_metric(ds,sim):
    """"
    implement your own metric here, and call it from sr_mcmc.model function
    ds is the dataset you compare to, sim is the simulation instance
    needs to return log likelihood of data given simulation
    """
    return




#example metric function
def baysianDistance(sr1, sr2, time_range=None, dt =1, debug = False):
    """
    Calculate the likelihood that the death times of sr1 are generated by sr2.
    convention is that sr1 is the data and sr2 is the simulation
    """
    from statsmodels.nonparametric.kernel_density import KDEMultivariate
    
    #if number of deathtimes is too small that causes issues and anyways not probable as a legitimate parameter set
    if len(sr2.getDeathTimes()) <= 5:
        return -np.inf
    
    death_times2 = sr2.getDeathTimes()
    events2 = sr2.events
    #if time range is not None, use only deathtimes withtin the time range
    if time_range is not None:
        events2 = events2[(death_times2 >= time_range[0]) & (death_times2 <= time_range[1])]
        death_times2 = death_times2[(death_times2 >= time_range[0]) & (death_times2 <= time_range[1])]
    #check that there are enough events to calculate the likelihood meaningfully
    if np.sum(events2) <= 5:
        return -np.inf
    
    death_times2 = death_times2[events2==1] #only those who died
    #this would be a smooth distribution generatated from sr2(simulation) to sample sr1(data) from
    kde = KDEMultivariate(death_times2, var_type='c', bw='normal_reference')
    
    
    events = sr1.events
    death_times = sr1.getDeathTimes()
    if time_range is not None:
        events = events[(death_times >= time_range[0]) & (death_times <= time_range[1])]
        death_times = death_times[(death_times >= time_range[0]) & (death_times <= time_range[1])]
    died = death_times[events==1]
    censored = death_times[events==0]
    ndied = len(died)
    p_death_before_t_end =np.sum(events2)/len(events2)
    
    
    #this piece of code is to accelerate the loglikelihood calculation
    times = np.linspace(0, max(died)+dt, int(np.ceil(max(died)+dt/dt)+1))
    log_pdt = kde.cdf(times) #the log integral of the probability density function on the time grid
    log_pdt = np.log(log_pdt[1:]-log_pdt[:-1])
    times = times[:-1]

    #if logp has nan values then try again then raise an error to debug
    if np.any(np.isnan(log_pdt)):
        if debug:
            print('log_pdt has nan values')
        return np.NaN

    logcdf = np.log(1-kde.cdf(times)) 
    #for every time in death times, find the nearst index in times and get the logp
    logps = 0
    logps_censored = 0
    for t in died:
        idx = np.argmin(np.abs(times-t))
        logps+=(log_pdt[idx])
        if debug:
            if log_pdt[idx] == -np.inf or np.isnan(log_pdt[idx]):
                print(f'log_pdt[{idx}] is {log_pdt[idx]}')

    for t in censored:
        idx = np.argmin(np.abs(times-t))
        logps_censored+=(logcdf[idx])

    #the liklihod given by the kde is L= p(t_death=x_i)|died before t_end) so to correct we need to multiply by the number of ndied/n
    #in the loglikelihood this gives a term of ndied*np.log(p_death_before_t_end)
    sums = logps+ndied*np.log(p_death_before_t_end) +logps_censored
    #check if sums is nan if so then raise an error to debug.
    if np.isnan(sums):
        print('ndied:',ndied)
        print('p_death_before_t_end:',p_death_before_t_end)
        print('logps:',logps)
        print('censored:',censored)
        print('censored_logLiklihood(censored):',logps_censored)
        raise ValueError('sums is nan')
    if debug:
        print('ndied:',ndied)
        print('p_death_before_t_end:',p_death_before_t_end)
        print('logps:',logps)
        print('censored:',censored)
        print('censored_logLiklihood(censored):',logps_censored)
        print('sums:',sums)
    return sums


def model(theta , n, nsteps, t_end, dataSet, sim=None, metric = 'baysian', time_range=None, time_step_multiplier = 1,parallel = False, dt=1, set_params=None, kwargs=None):
    #implement your model here. extra parameters that are passed to the sampler can be used here with kwargs as dictionary
    #if you passed extra parameteers to sr_mcmc.getSampler, as in sr_mcmc.getSampler(.....,bob='men',cost=3), then kwargs 
    #will be a dictionary {'bob':'men','cost':3}
    #The function accepts the parameters of the SR model and returns score according to the metric.

    if set_params is None:
        set_params = {}
    # parse parameters
    pv = srmc.parse_theta(theta, set_params)
    eta = pv['eta']
    beta = pv['beta']
    epsilon = pv['epsilon']
    xc = pv['xc']
    external_hazard = pv['external_hazard']
    return



def example_model(theta , n, nsteps, t_end, dataSet, sim=None, metric = 'baysian', time_range=None, time_step_multiplier = 1,parallel = False, dt=1, set_params=None, kwargs=None):
    """
    The function accepts the parameters of the SR model and returns score according to the metric.
    """
    if set_params is None:
        set_params = {}
    # parse parameters
    pv = srmc.parse_theta(theta, set_params)
    eta = pv['eta']
    beta = pv['beta']
    epsilon = pv['epsilon']
    xc = pv['xc']
    external_hazard = pv['external_hazard']
    theta_sr = np.array([eta, beta, epsilon, xc])
    time_step_size = t_end/(nsteps*time_step_multiplier)
    if 1/beta < time_step_size:
        return -np.inf
    sim = srmc.getSr(theta_sr, n, nsteps, t_end, external_hazard = external_hazard, time_step_multiplier=time_step_multiplier,parallel=parallel) if sim is None else sim
    
    import SRmodellib as sr
    tprob =  sr.distance(dataSet,sim,metric=metric,time_range=time_range, dt=dt)
    if np.any(np.isnan(tprob)):
        return -np.inf

    return tprob


#method without parallelization (for cluster usage)
@jit(nopython=jit_nopython)
def death_times_accelerator(s,dt,t,eta,beta,kappa,epsilon,xc,sdt,npeople,external_hazard = np.inf, time_step_multiplier = 1):
    death_times = []
    events = []
    for i in range(npeople):
        x=0
        j=0
        ndt = dt/time_step_multiplier
        nsdt = sdt/np.sqrt(time_step_multiplier)
        chance_to_die_externally = np.exp(-external_hazard)*ndt
        while j in range(s-1) and x<xc:
            for i in range(time_step_multiplier):
                noise = np.sqrt(2*epsilon)*np.random.normal(loc = 0,scale = 1)
                x = x+ndt*(eta*(t[j]+i*ndt)-beta*x/(x+kappa))+noise*nsdt
                x = np.maximum(x, 0)
                if np.random.uniform(0,1)<chance_to_die_externally:
                    x = xc
                if x>=xc:
                    break
            j+=1
        if x>=xc:
            death_times.append(j*dt)
            events.append(1)
        else:
            death_times.append(j*dt)
            events.append(0)

    return death_times, events

##method with parallelization (run on your computer)
def death_times_accelerator2(s,dt,t,eta,beta,kappa,epsilon,xc,sdt,npeople,external_hazard = np.inf,time_step_multiplier = 1):
    @jit(nopython=jit_nopython)
    def calculate_death_times(npeople, s, dt, t, eta, beta, kappa, epsilon, xc, sdt, external_hazard,time_step_multiplier):
        death_times = []
        events =[]
        for i in range(npeople):
            died = False
            x = 0
            j = 0
            ndt = dt/time_step_multiplier
            nsdt = np.sqrt(ndt)
            chance_to_die_externally = np.exp(-external_hazard)*ndt
            while j in range(s - 1) and x < xc and not died:
                for i in range(time_step_multiplier):
                    noise = np.sqrt(2*epsilon)*np.random.normal(loc = 0,scale = 1)
                    x = x+ndt*(eta*(t[j]+i*ndt)-beta*x/(x+kappa))+noise*nsdt
                    x = np.maximum(x, 0)
                    if np.random.uniform(0,1)<chance_to_die_externally:
                        x = xc
                    if x>=xc:
                        died = True
                j += 1
            if died:
                death_times.append(j * dt)
                events.append(1)
            else:
                death_times.append(j * dt)
                events.append(0)
        return death_times, events

    n_jobs = os.cpu_count()
    npeople_per_job = npeople // n_jobs
    results = Parallel(n_jobs=n_jobs)(delayed(calculate_death_times)(
        npeople_per_job, s, dt, t, eta, beta, kappa, epsilon, xc, sdt, external_hazard, time_step_multiplier
    ) for _ in range(n_jobs))

    death_times = [dt for sublist in results for dt in sublist[0]]
    events = [event for sublist in results for event in sublist[1]]
    return death_times, events